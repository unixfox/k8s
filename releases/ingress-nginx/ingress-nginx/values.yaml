defaultBackend:
  enabled: true
  image:
    registry: k8s.gcr.io
    image: defaultbackend-arm64
controller:

  # # -- Containers, which are run before the app containers are started.
  # extraInitContainers:
  #   - name: copy-secrets
  #     image: bitnami/kubectl
  #     - /bin/bash
  #     - -c
  #     - set -e -x;
  #       echo "$(./kubectl get secrets -n invidious --field-selector metadata.name=chart-invidious-tls -o json -o=jsonpath="{.items[0].data.tls\.key}" | base64 -d)" \
  #       $'\n'"$(./kubectl get secrets -n invidious --field-selector metadata.name=chart-invidious-tls -o json -o=jsonpath="{.items[0].data.tls\.crt}" | base64 -d)" > \
  #       /var/tmp/haproxy-temp/full.pem;

  admissionWebhooks:
    enabled: false
  image:
    registry: quay.io
    image: unixfox/ingress-nginx-controller-dynamic-modules
  # tag: v1.5.1
    digest: ""
  watchIngressWithoutClass: true
  terminationGracePeriodSeconds: 0
  dnsPolicy: ClusterFirstWithHostNet
  metrics:
    port: 10254
    enabled: true
  podAnnotations:
    prometheus.io/port: "10254"
  config:
    enable-brotli: "true"
    ssl-protocols: "TLSv1.2 TLSv1.3"
    ssl-ciphers: "ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384"
    hsts-preload: "true"
    disable-access-log: "false"
    error-log-level: "error"
    use-gzip: "true"
    lua-shared-dicts: "limit_api_captions: 100, limit_api_storyboards: 100, limit_api_subscriptions: 100, limit_feed_channel: 100, limit_login: 100, limit_videoplayback: 100, limit_main_routes: 100, limit_latest_version: 100, limit_webhooks: 100, limit_search_searx: 100, limit_image_req: 100"
    worker-shutdown-timeout: "0s"
    http-snippet: |
      js_shared_dict_zone zone=antibotnitter:1M timeout=7200s evict;
      geo $from_cluster {
        default    0;
        10.10.0.0/16 1;
        10.43.0.0/16 1;
      }
      upstream upstreamnitter {
        server nitter.nitter.svc.cluster.local:8080;
        server nitter.nitter.svc.cluster.local:8080;
        server nitter.nitter.svc.cluster.local:8080;    
      }
#      limit_conn_zone $binary_remote_addr zone=addrvideoplayback:10m;
#     limit_traffic_rate_zone rate $binary_remote_addr 32m;

    #log-format-upstream: '$remote_addr - $remote_user [$time_local] "$request" $status $body_bytes_sent "$http_referer" "$http_user_agent"'

    #log-format-upstream: '$remote_addr - $remote_user [$time_local] "$request" $status $body_bytes_sent "$http_referer" "$http_user_agent" - $request_headers'
    #log-format-upstream: "$remote_addr - $request - $status - $http_referer - $http_user_agent - $request_headers"
    #log-format-upstream: 'verb="$request_method" request="$uri" response=$status clientip="$remote_addr" scheme="$scheme" bytes=$bytes_sent agent="$http_user_agent" referrer="$http_referer" request_time=$request_time upstream_response=$upstream_status upstream_response_time=$upstream_response_time resource_name="$resource_name" resource_namespace="$resource_namespace" resource_type="$resource_type" http_host="$http_host" request_id="$request_id" time_date="$time_iso8601" request_headers="$request_headers"'
    log-format-upstream: '{"verb": "$request_method","uri": "$uri", "request_uri": "$request_uri", "request": "$request","response": "$status","clientip": "$remote_addr","scheme": "$scheme","bytes": "$bytes_sent","agent": "$http_user_agent","referrer": "$http_referer","request_time": "$request_time","upstream_response": "$upstream_status","upstream_response_time": "$upstream_response_time","http_host": "$http_host","service_name": "$service_name", "ingress_name": "$ingress_name", "status": "$status","request_id": "$request_id","time_date": "$time_iso8601","request_headers": "$request_headers","remote_port": "$remote_port"}'
    log-format-escape-json: 'true'

    location-snippet: |
      add_header Alt-Svc 'h2="ingress.yewtu.be:443"; ma=86400';

    block-cidrs: 37.46.114.0/24,93.143.17.28

    main-snippet:
      load_module modules/ngx_http_js_module.so;

    server-snippet: |
      listen [::]:8444 ssl proxy_protocol;
      listen [::]:8445 proxy_protocol;
      set_real_ip_from 2605:6400:30:ee66::/128;
      real_ip_header proxy_protocol;
      set_by_lua_block $request_headers{
        local h = ngx.req.get_headers()
        local request_headers_all = ""
        for k, v in pairs(h) do
          local rowtext = ""
          rowtext = string.format("[%s %s]\n", k, v)
          request_headers_all = request_headers_all .. rowtext

        end
        return request_headers_all
      }
  lifecycle:
    postStart:
      exec:
        command: ["/bin/sh", "-c", "touch /tmp/proxy-temp/passedchallengeips.txt"]
    preStop:
      exec:
        command:
          - /bin/echo
  extraVolumes:
  - name: ingress-nginx-controller
    configMap:
      name: ingress-nginx-controller
  - name: sharedsock
    hostPath:
      path: /tmp/http-socket
  - name: cache-proxy-temp
    emptyDir:
      medium: "Memory"
  extraVolumeMounts:
  - mountPath: /etc/nginx/lua/count.lua
    name: ingress-nginx-controller
    subPath: count.lua
  - mountPath: /usr/local/nginx/anti_ddos_challenge.lua
    name: ingress-nginx-controller
    subPath: anti_ddos_challenge.lua
  - mountPath: /etc/nginx/antibotsimple.js
    name: ingress-nginx-controller
    subPath: antibotsimple.js
  - name: sharedsock
    mountPath: /app/socket
  - name: cache-proxy-temp
    mountPath: /tmp/proxy-temp

  kind: DaemonSet
  hostNetwork: true
  tolerations:
  - key: "node.kubernetes.io/unreachable"
    operator: "Exists"
    effect: "NoExecute"
    tolerationSeconds: 5
  - key: "node.kubernetes.io/not-ready"
    operator: "Exists"
    effect: "NoExecute"
    tolerationSeconds: 5
  - key: app-nextcloud
    operator: Equal
    value: "true"
    effect: NoExecute
  - key: dedicated-server
    operator: Equal
    value: "true"
    effect: NoExecute
  service:
    type: ClusterIP
    enabled: true